@InProceedings{rajkumar12,
  title = {A Differentially Private Stochastic Gradient Descent Algorithm for Multiparty Classification},
  author = {Arun Rajkumar and Shivani Agarwal},
  pages = {933--941},
  abstract = {We consider the problem of developing privacy preserving machine learning algorithms in a distributed multiparty setting. Here different parties own different parts of a data set, and the goal is to learn a classifier from the entire data set without any party revealing any information about the individual data points it owns. Pathak et al (2010) recently proposed a solution to this problem in which each party learns a local classifier from its own data, and a third party then aggregates these classifiers in a privacy-preserving manner using a cryptographic scheme. The generalization performance of their algorithm is sensitive to the number of parties and the relative fractions of data owned by the different parties. In this paper, we describe a new differentially private algorithm for the multiparty setting that uses a stochastic gradient descent based procedure to directly optimize the overall multiparty objective rather than combining classifiers learned from optimizing local objectives. The algorithm achieves a slightly weaker form of differential privacy than that of Pathak et al (2010), but provides improved generalization guarantees that do not depend on the number of parties or the relative sizes of the individual data sets. Experimental results corroborate our theoretical findings.},
  pdf = {/v22/rajkumar12/rajkumar12.pdf},
  supplementary = {Supplementary:/v22/rajkumar12/rajkumar12Supple.pdf},
}
