@InProceedings{gonzalez09a,
  title = {Residual Splash for Optimally Parallelizing Belief Propagation},
  author = {Joseph Gonzalez and Yucheng Low and Carlos Guestrin},
  pages = {177--184},
  abstract = {As computer architectures move towards parallelism we must build a    new theoretical understanding of parallelism in machine learning.    In this paper we focus on parallelizing message passing inference    algorithms in graphical models. We develop a theoretical    understanding of the limitations of parallelism in belief    propagation and bound the optimal achievable running parallel    performance on a certain class of graphical models.  We demonstrate    that the fully synchronous parallelization of belief propagation is    highly inefficient.  We provide a new parallel belief propagation    which achieves optimal performance on a certain class of graphical    models.  Using two challenging real-world problems, we empirically    evaluate the performance of our algorithm. On the real-world    problems, we find that our new algorithm achieves near linear    performance improvements and out performs alternative parallel    belief propagation algorithms.},
  pdf = {/v5/gonzalez09a/gonzalez09a.pdf},
}
