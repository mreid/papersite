@InProceedings{helmbold12,
  title = {New Bounds for Learning Intervals with Implications for Semi-Supervised Learning},
  author = {David   P. Helmbold and Philip M. Long},
  pages = {30.1--30.15},
  abstract = {We study learning of initial intervals in the prediction model. We show that for each distribution \emph{D} over the domain, there is an algorithm \emph{A_{D}}, whose probability of a mistake in round m is at most \emph{(½ + o(1))/m}. We also show that the best possible bound that can be achieved in the case in which the same algorithm \emph{A} must be applied for all distributions \emph{D} is at least (^{1}⁄_{√\emph{e}} - o(1))^{1}⁄_{\emph{m}} > (^{3}⁄_{5}-o(1))^{1}⁄_{\emph{m}}. Informally, ``knowing'' the distribution \emph{D} enables an algorithm to reduce its error rate by a constant factor strictly greater than 1. As advocated by Ben-David et al. (2008), knowledge of \emph{D} can be viewed as an idealized proxy for a large number of unlabeled examples.},
  pdf = {/v23/helmbold12/helmbold12.pdf},
}
