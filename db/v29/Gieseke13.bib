@inproceedings{Gieseke13,
  pdf = {/v29/Gieseke13.pdf},
  title = {Polynomial Runtime Bounds for Fixed-Rank Unsupervised Least-Squares Classification},
  author = {Gieseke, Fabian and Pahikkala, Tapio and Igel, Christian},
  pages = {62-71},
  abstract = {Maximum margin clustering can be regarded as the direct extension of support vector machines to unsupervised learning scenarios. The goal is to partition unlabeled data into two classes such that a subsequent application of a support vector machine would yield the overall best result (with respect to the optimization problem associated with support vector machines). While being very appealing from a conceptual point of view, the combinatorial nature of the induced optimization problem renders a direct application of this concept difficult. In order to obtain efficient optimization schemes, various surrogates of the original problem definition have been proposed in the literature. In this work, we consider one of these variants, called unsupervised regularized least-squares classification, which is based on the square loss, and develop polynomial upper runtime bounds for the induced combinatorial optimization task. In particular, we show that for $n$ patterns and kernel matrix of fixed rank $r$ (with given  eigendecomposition), one can obtain an optimal solution in $\mathcal{O}(n^{r})$ time for $r \leq 2$ and in $\mathcal{O}(n^{r-1})$ time for $r\geq 3$. The algorithmic framework is based on an interesting connection to the field of quadratic zero-one programming and permits the computation of exact solutions for the more general case of non-linear kernel functions in polynomial time.},
}
