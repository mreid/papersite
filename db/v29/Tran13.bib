@inproceedings{Tran13,
  pdf = {http://jmlr.org/proceedings/papers/v29/Tran13.pdf},
  title = {Improving Predictive Specificity of Description Logic Learners by Fortification},
  author = {Tran, An and Dietrich, Jens and Guesgen, Hans and Marsland, Stephen},
  pages = {419-434},
  abstract = {The predictive accuracy of a learning algorithm can be split into specificity and sensitivity, amongst other decompositions. Sensitivity, also known as completeness, is the ratio of true positives to the total number of positive examples, while specificity is the ratio of true negative to the total negative examples. In top-down learning methods of inductive logic programming, there is generally a bias towards sensitivity, since the learning starts from the most general rule (everything is positive) and specialises by excluding some of the negative examples. While this is often useful, it is not always the best choice: for example, in novelty detection, where the negative examples are rare and often varied, they may well be ignored by the learning. In this paper we introduce a method that attempts to remove the bias towards sensitivity by fortifying the model by computing and then including in the model some descriptions of the negative data even if they are considered redundant by the normal learning algorithm. We demonstrate the method on a set of standard datasets for description logic learning and show that the predictive accuracy increases.},
}
