@InProceedings{zhang10c,
  title = {Multi-Task Learning using Generalized t Process},
  author = {Yu Zhang and Ditâ€“Yan Yeung},
  pages = {964--971},
  abstract = {Multi-task learning seeks to improve the generalization performance of a learning task with the help of other related learning tasks.  Among the multi-task learning methods proposed thus far, Bonilla et al.'s method provides a novel multi-task extension of Gaussian process (GP) by using a task covariance matrix to model the relationships between tasks. However, learning the task covariance matrix directly has both computational and representational drawbacks. In this paper, we propose a Bayesian extension by modeling the task covariance matrix as a random matrix with an inverse-Wishart prior and integrating it out to achieve Bayesian model averaging. To make the computation feasible, we first give an alternative weight-space view of Bonilla et al.'s multi-task GP model and then integrate out the task covariance matrix in the model, leading to a multi-task generalized t process (MTGTP). For the likelihood, we use a generalized t noise model which, together with the generalized t process prior, brings about the robustness advantage as well as an analytical form for the marginal likelihood.  In order to specify the inverse-Wishart prior, we use the maximum mean discrepancy (MMD) statistic to estimate the parameter matrix of the inverse-Wishart prior. Moreover, we investigate some theoretical properties of MTGTP, such as its asymptotic analysis and learning curve. Comparative experimental studies on two common multi-task learning applications show very promising results.},
  pdf = {/v9/zhang10c/zhang10c.pdf},
  supplementary = {Supplementary:/v9/zhang10c/zhang10cSupple.pdf},
}
