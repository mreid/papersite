@InProceedings{orabona16,
  author = {Orabona, Francesco and P\'al, D\'avid},
  title = {Open Problem: Parameter-Free and Scale-Free Online Algorithms},
  pages = {1659-1664},
  abstract = {Existing vanilla algorithms for online linear optimization have $O((\eta R(u) +
1/\eta) \sqrt{T})$ regret with respect to any competitor $u$, where $R(u)$ is a
$1$-strongly convex regularizer and $\eta > 0$ is a tuning parameter of the
algorithm. For certain decision sets and regularizers, the so-called
\emph{parameter-free} algorithms have $\widetilde O(\sqrt{R(u) T})$ regret with
respect to any competitor $u$.  Vanilla algorithm can achieve the same bound
only for a fixed competitor $u$ known ahead of time by setting $\eta =
1/\sqrt{R(u)}$. A drawback of both vanilla and parameter-free algorithms is that
they assume that the norm of the loss vectors is bounded by a constant known to
the algorithm. There exist \emph{scale-free} algorithms that have $O((\eta R(u) +
1/\eta) \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t})$ regret with respect to any
competitor $u$ and for any sequence of loss vector $\ell_1, \dots, \ell_T$.
Parameter-free analogue of scale-free algorithms have never been designed. Is is
possible to design algorithms that are simultaneously \emph{parameter-free} and
\emph{scale-free}?
},
  section = {open},
}
