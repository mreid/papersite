@InProceedings{Scott13,
  pdf = {http://jmlr.org/proceedings/papers/v30/Scott13.pdf},
  title = {Classification with Asymmetric Label Noise: Consistency and Maximal Denoising},
  author = {Clayton Scott and Gilles Blanchard and Gregory Handy},
  pages = {489-511},
  abstract = {In many real-world classification problems, the labels of training examples are randomly corrupted. Thus, the set of training examples for each class is contaminated by examples of the other class. Previous theoretical work on this problem assumes that the two classes are separable, that the label noise is independent of the true class label, or that the noise proportions for each class are known. We introduce a general framework for classification with label noise that eliminates these assumptions. Instead, we give assumptions ensuring identifiability and the existence of a consistent estimator of the optimal risk, with associated estimation strategies. For any arbitrary pair of contaminated distributions, there is a unique pair of non-contaminated distributions satisfying the proposed assumptions, and we argue that this solution corresponds in a certain sense to maximal denoising. In particular, we find that learning in the presence of label noise is possible even when the class-conditional distributions overlap and the label noise is not symmetric. A key to our approach is a universally consistent estimator of the maximal proportion of one distribution that is present in another, a problem we refer to as``mixture proportion estimation. This work is motivated by a problem in nuclear particle classification.},
}
