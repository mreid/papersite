@InProceedings{acharya12a,
  pdf = {http://jmlr.org/proceedings/papers/v27/acharya12a.pdf},
  title = {Transfer Learning with Cluster Ensembles},
  author = {A. Acharya and E. R. Hruschka and J. Ghosh and S. Acharyya},
  pages = {123--132},
  abstract = {Traditional supervised learning algorithms typically assume that the training data and test data come from a common underlying distribution. Therefore, they are challenged by the mismatch between training and test distributions encountered in transfer learning situations. The problem is further exacerbated when the test data actually comes from a different domain and contains no labeled example. This paper describes an optimization framework that takes as input one or more classifiers learned on the source domain as well as the results of a cluster ensemble operating solely on the target domain, and yields a consensus labeling of the data in the target domain. This framework is fairly general in that it admits a wide range of loss functions and classification/clustering methods. Empirical results on both text and hyperspectral data indicate that the proposed method can yield superior classification results compared to applying certain other transductive and transfer learning techniques or na\"{\i}vely applying the classifier (ensemble) learnt on the source domain to the target domain.},
}
