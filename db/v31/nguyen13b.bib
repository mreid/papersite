@InProceedings{nguyen13b,
  pdf = {/v31/nguyen13b.pdf},
  supplementary = {Supplementary:nguyen13b-supp.pdf},
  title = {Efficient Variational Inference for Gaussian Process Regression Networks},
  author = {Nguyen, Trung and Bonilla, Edwin},
  pages = {472--480},
  abstract = {In multi-output regression applications the correlations between the response variables may vary with the input space and can be highly non-linear.  Gaussian process regression networks (GPRNs)  are flexible  and effective models to represent such complex adaptive output dependencies. However, inference in GPRNs is  intractable.  In this paper we propose two efficient variational inference methods for  GPRNs. The first method, GPRN-MF, adopts a mean-field approach with full Gaussians over the GPRN's parameters as its factorizing distributions. The second method, GPRN-NPV, uses a nonparametric  variational inference approach. We derive analytical forms for the evidence lower bound on both methods, which we use to learn the variational parameters and the hyper-parameters of the GPRN model. We obtain closed-form updates for the parameters of  GPRN-MF  and show that, while having relatively complex approximate posterior distributions, our approximate methods require the estimation of  O(N) variational parameters rather than O(N2) for the  parameters' covariances.  Our experiments on real data sets show that GPRN-NPV  may give a better approximation to the posterior distribution compared to GPRN-MF,  in terms of both predictive performance and stability. },
}
