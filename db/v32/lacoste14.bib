@InProceedings{lacoste14,
  pdf = {/v32/lacoste14.pdf},
  section = {cycle-1},
  supplementary = {Supplementary:lacoste14-supp.pdf},
  title = {Agnostic Bayesian Learning of Ensembles},
  author = {Alexandre Lacoste and Mario Marchand and Fran√ßois Laviolette and Hugo Larochelle},
  pages = {611-619},
  abstract = {We propose a method for producing ensembles of predictors based on holdout estimations of their generalization performances. This approach uses a prior directly on the performance of predictors taken from a finite set of candidates and attempts to infer which one is best. Using Bayesian inference, we can thus obtain a posterior that represents our uncertainty about that choice and construct a weighted ensemble of predictors accordingly. This approach has the advantage of not requiring that the predictors be probabilistic themselves, can deal with arbitrary measures of performance and does not assume that the data was actually generated from any of the predictors in the ensemble. Since the problem of finding the best (as opposed to the true) predictor among a class is known as agnostic PAC-learning, we refer to our method as agnostic Bayesian learning. We also propose a method to address the case where the performance estimate is obtained from k-fold cross validation. While being efficient and easily adjustable to any loss function, our experiments confirm that the agnostic Bayes approach is state of the art compared to common baselines such as model selection based on k-fold cross-validation or a linear combination of predictor outputs.},
}
