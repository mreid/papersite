@INPROCEEDINGS{xiong14,
  author = {Xiong, Hanchen and Szedmak, Sandor and Piater, Justus},
  title = {Towards Maximum Likelihood: Learning Undirected Graphical Models
	using Persistent Sequential Monte Carlo},
  pages = {205-220},
  abstract = {Along with the emergence of algorithms such as persistent contrastive
	divergence (PCD), tempered transition and parallel tempering, the
	past decade has witnessed a revival of learning undirected graphical
	models (UGMs) with sampling-based approximations. In this paper,
	based upon the analogy between Robbins-Monro's stochastic approximation
	procedure and sequential Monte Carlo (SMC), we analyze the strengths
	and limitations of state-of-the-art learning algorithms from an SMC
	point of view. Moreover, we apply the rationale further in sampling
	at each iteration, and propose to learn UGMs using persistent sequential
	Monte Carlo (PSMC). The whole learning procedure is based on the
	samples from a long, persistent sequence of distributions which are
	actively constructed. Compared to the above-mentioned algorithms,
	one critical strength of PSMC- based learning is that it can explore
	the sampling space more effectively. In particular, it is robust
	when learning rates are large or model distributions are high-dimensional
	and thus multi-modal, which often causes other algorithms to deteriorate.
	We tested PSMC learning, also with other related methods, on carefully-designed
	experiments with both synthetic and real-word data, and our empirical
	results demonstrate that PSMC compares favorably with the state of
	the art.},
}
