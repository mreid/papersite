@InProceedings{anandkumar16,
  author = {Anandkumar, Animashree and Ge, Rong},
  title = {Efficient approaches for escaping higher order saddle points  in non-convex optimization},
  pages = {81-102},
  abstract = {Local search heuristics for non-convex optimizations are popular   in applied machine learning. However, in general it is  hard to  guarantee that such algorithms even  converge to a {\em local minimum}, due to the existence of complicated saddle point structures in high dimensions. Many functions have {\em degenerate} saddle points such that the first and second order derivatives cannot distinguish them with local optima.  In this paper we use higher order derivatives to escape these saddle points: we design the first efficient algorithm  guaranteed to converge to a third order local optimum (while existing techniques are at most second order). We also show that it is NP-hard to extend this further to finding fourth order local optima.},
}
