@InProceedings{banerjee13a,
  pdf = {http://jmlr.org/proceedings/papers/v31/banerjee13a.pdf},
  title = {Bayesian learning of joint distributions of objects},
  section = {notable},
  note = {Notable paper award},
  author = {Banerjee, Anjishnu and Murray, Jared  and Dunson, David},
  pages = {1--9},
  abstract = {There is increasing interest in broad application areas in defining flexible joint models for data having a variety of measurement scales, while also allowing data of complex types, such as functions, images and documents. We consider a general framework for nonparametric Bayes joint modeling through mixture models that incorporate dependence across data types through a joint mixing measure.  The mixing measure is assigned a novel infinite tensor factorization (ITF) prior that allows flexible dependence in cluster allocation across data types.  The ITF prior is formulated as a tensor product of stick-breaking processes.  Focusing on a convenient special case corresponding to a Parafac factorization, we provide basic theory justifying the flexibility of the proposed prior.  Focusing on ITF mixtures of product kernels, we develop a  new Gibbs sampling algorithm for routine implementation relying on slice sampling. The methods are compared with alternative joint mixture models based on Dirichlet processes and related approaches through simulations and real data applications.},
}
