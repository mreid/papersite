@InProceedings{guillory09a,
  title = {Active Learning as Non-Convex Optimization},
  author = {Andrew Guillory and Erick Chastain and Jeff Bilmes},
  pages = {201--208},
  abstract = {We propose a new view of active learning algorithms as optimization. We show that many online active learning algorithms can be viewed as stochastic gradient descent on non-convex objective functions. Variations of some of these algorithms and objective functions have been previously proposed without noting this connection.  We also point out a connection between the standard min-margin offline active learning algorithm and non-convex losses.  Finally, we discuss and show empirically how viewing active learning as non-convex loss minimization helps explain two previously observed phenomena: certain active learning algorithms achieve better generalization error than passive learning algorithms on certain data sets and on other data sets many active learning algorithms are prone to local minima.},
  pdf = {http://jmlr.org/proceedings/papers/v5/guillory09a/guillory09a.pdf},
}
